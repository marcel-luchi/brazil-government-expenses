import os
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
from helpers.spark_utils import SparkUtils
from helpers import Schemas
from helpers.constants import VOUCHERS_DIR, STAGING_DIR, \
    DIMENSION_DIR, RAW_DIR, VOUCHERS_FILENAME


class VouchersDimensionRawToStage(BaseOperator):
    """This class transforms data from JSON files generated by reading the Portal da Transparencia
       API from Brazil's government.

       Extracted data is for purchases payments realized with government corporate credit cards.

       Args:
           bucket (str): Bucket in which source data is stored and target data will be written.
           table (str): Name of the table that will be loaded.
    """
    ui_color = '#80BD9E'

    @apply_defaults
    def __init__(self,
                 bucket,
                 table,
                 *args, **kwargs):
        super(VouchersDimensionRawToStage, self).__init__(*args, **kwargs)
        self.bucket = bucket
        self.table = table
        self.csv_properties = {'header': True,
                               'encoding': 'ISO-8859-1',
                               'sep': ';'}

    def __stage_dimensions(self, spark, params):
        """Creates staging tables from JSON portal da transparencia files."""
        year = params.get('year')
        month = params.get('month')
        path = os.path.join(self.bucket,
                            RAW_DIR,
                            VOUCHERS_DIR,
                            f"{params.get('file')}{year}-{month}*")

        dimension_path = os.path.join(self.bucket,
                                      DIMENSION_DIR,
                                      params.get('table'))
        staging_path = os.path.join(self.bucket,
                                    STAGING_DIR,
                                    params.get('output_file'))

        self.log.info("Reading input files:")
        self.log.info(path)
        self.log.info("Writing output to:")
        self.log.info(staging_path)

        df_data = spark.read.json(path, schema=Schemas.voucher_schema).selectExpr(params.get('schema'))

        try:
            df_dimension = spark.read.parquet(dimension_path)
            self.log.info(f'Dimension found, has {df_dimension.count()} records')

        except:
            df_dimension = None
            self.log.info('Dimension not found.')

        if df_dimension is None:
            df_data.dropDuplicates(params.get('table_key')).dropna(subset=params.get('table_key'))\
                .write.parquet(staging_path, mode='overwrite')

        else:
            df_data.dropDuplicates(params.get('table_key')).dropna(subset=params.get('table_key')) \
                .join(df_dimension, params.get('table_key'), how='leftanti')\
                .write.parquet(staging_path, mode='overwrite')

    def execute(self, context):
        """Method called by Airflow Task."""
        year = '{execution_date.year}'.format(**context)
        month = '{execution_date.month}'.format(**context).zfill(2)
        spark = SparkUtils.create_spark_session()
        extractions = {'vendor': {'params': {'table': self.table,
                                             'output_file': 'vouchers_vendor',
                                             'year': year,
                                             'month': month,
                                             'schema': Schemas.voucher_vendor_columns,
                                             'table_key': Schemas.vendor_keys,
                                             'file': VOUCHERS_FILENAME
                                             }},
                       'agency': {'params': {'table': self.table,
                                             'output_file': 'vouchers_agency',
                                             'year': year,
                                             'month': month,
                                             'schema': Schemas.voucher_agency_columns,
                                             'table_key': Schemas.agency_keys,
                                             'file': VOUCHERS_FILENAME
                                             }},
                       'city': {'params': {'table': self.table,
                                           'output_file': 'vouchers_city',
                                           'year': year,
                                           'month': month,
                                           'schema': Schemas.voucher_city_columns,
                                           'table_key': Schemas.city_keys,
                                           'file': VOUCHERS_FILENAME
                                           }}
                       }

        params = extractions.get(self.table).get('params')
        self.__stage_dimensions(spark, params)
